<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">NEURONSTAR</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml" />
<link rel="alternate" type="text/html" href="http://localhost:4000" />
<updated>2017-09-02T01:40:30+00:00</updated>
<id>http://localhost:4000/</id>
<author>
  <name>HLLM</name>
  <uri>http://localhost:4000/</uri>
  
</author>


<entry>
  <title type="html"><![CDATA[贝叶斯统计、罕见疾病和抽奖问题]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/sciencecn/bayesian/" />
  <id>http://localhost:4000/sciencecn/bayesian</id>
  <updated>2015-05-10 00:00:00 +0000T00:00:00-00:00</updated>
  <published>2015-05-10T00:00:00+00:00</published>
  
  <author>
    <name>neuronstar</name>
    <uri>http://localhost:4000</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;概率论的核心的概念应该是概率了。基于我们对于逻辑事件的理解，下面的式子成立，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A\cup B) = P(A) + P(B) - P(A\cap B),&lt;/script&gt;

&lt;p&gt;也就是说，A 或者 B 同时发生的概率，等于 A 发生的概率加上 B 发生的概率，再减掉 A 和 B 同时发生的概率，因为前面 A 发生的概率加上 B 发生的概率包含了两个 A 和 B 同时发生的概率。&lt;sup id=&quot;fnref:set&quot;&gt;&lt;a href=&quot;#fn:set&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;figcaption&gt;
Venn diagram：事件 A 或 B 是所有的带有颜色区域，而事件 A 加上事件 B，是所有带有颜色的区域，但是交叠部分被数了两次。
&lt;/figcaption&gt;
  &lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/bayesian/venn-diagram-AB.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;我们定义 $P(A,B) = P(A\cap B)$ （joint probability）。这样有两种比较有趣的特殊情况：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;当 $P(A,B)=0$ 也就是说，A 事件和 B 事件没有任何交集的时候，事件 A 或者事件 B 发生的概率就简化成了&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A\cup B) = P(A) + P(B).&lt;/script&gt;

    &lt;p&gt;比如说，一件物体在自然光下此时此刻是红色或者绿色的概率，就等于这件物体是红色的概率加上这件物体是绿色的概率，因为这件物体在自然光下不能同时既是红色又是绿色。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当 $P(A,B)=P(A)P(B)$ 的时候，我们称 A，B 这两件事情是统计上独立的事件，因为这个定义显示，发生 A 的概率和发生 B 的概率他们没有关联。作为一个对照，两件事情相互关联有很多可能性，比如 $P(A,B)=P(A)(1-P(A))$。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bayes-定理&quot;&gt;Bayes 定理&lt;/h2&gt;

&lt;p&gt;为什么要考虑事件之间的逻辑关系呢？因为理解 Bayes 定理需要这些逻辑关系。&lt;/p&gt;

&lt;p&gt;我们考虑两个组事件 A、B，我们使用符号 $P(A\vert B)$ 来表示发生事件 B 的前提下发生事件 A 的概率，这类似一个对 $P(A)$ 的重新归一化。我们思考这样一个表达式：$P(A\vert B)P(B)$，它的意思是说，发生了事件 B 的前提下发生 A 的概率，乘以发生事件 B 的概率。&lt;sup id=&quot;fnref:condi2&quot;&gt;&lt;a href=&quot;#fn:condi2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 这其实就是发生事件 A 和 B 的概率，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A\vert B)P(B) = P(A\cap B).&lt;/script&gt;

&lt;p&gt;同样的道理，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B\vert A)P(A)=P(A\cap B).&lt;/script&gt;

&lt;p&gt;这两个式子的右边都相同，也就是说我们有这样一个对称性，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A\vert B)P(B) = P(B\vert A)P(A).&lt;/script&gt;

&lt;p&gt;上面这个看似简单直白的关系就是 Bayes 定理。&lt;/p&gt;

&lt;p&gt;作为一个小小的推广，我们可以扩展到多个集合的情况，类似于上面的式子，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B) = \sum_{i=1}^N P(B\vert A_i)P(A_i),&lt;/script&gt;

&lt;p&gt;$P(A_i)$ 被称作 priori,先验概率。这样我们可以用类似的手法得到 Bayes 定理，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A_i\vert B) \sum_{k=1}^N P(B\vert A_k)P(A_k) = P(B\vert A_i)P(A_i).&lt;/script&gt;

&lt;p&gt;下面用两个例子来解释 Bayes 定理。&lt;/p&gt;

&lt;h2 id=&quot;罕见疾病测试&quot;&gt;罕见疾病测试&lt;/h2&gt;

&lt;p&gt;设想有一种疾病：&lt;sup id=&quot;fnref:raredisease&quot;&gt;&lt;a href=&quot;#fn:raredisease&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;它在整个人群中的发病率只有 0.001，记作 $P(D)$。&lt;/li&gt;
  &lt;li&gt;医生们发明了一种方法来检测这种疾病，对于一个患病者来说，检测出来的准确率高达 99%，这个概率记作 $P(+\vert D)$，即在被检测者是患病者的前提下，检测结果为阳性的概率。&lt;/li&gt;
  &lt;li&gt;同时这种方法也有一个很小的概率会把一个不患病的人检测为阳性，这个概率小到了 0.005，记作 $P(+\vert H)$，即健康的人被检测为结果阳性的概率。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有了这些数据，我们可以计算在整个人群中，随便找出一个人，检测结果为阳性的概率：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(+) = P(+\vert D)P(D)+ P(+\vert H)P(H)=0.0060.&lt;/script&gt;

&lt;p&gt;我们更关系的结果是，如果我们有一个检测结果为阳性，那么这个人是患病的概率。这里我们可以使用上面的结果加上 Bayes 定理来计算。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D\vert +) = P(+\vert D)P(D)/P(+) = 0.99\times 0.001/0.0060 = 0.165.&lt;/script&gt;

&lt;p&gt;也就是说明，我们用这种方法做普查，在人群中某个人的检测结果是阳性，但是这个人真的患病的概率只有 16.5%。这样的检测结果为阳性，这个人患病的概率却如此小，说明这种方法没有单独使用的意义。&lt;/p&gt;

&lt;p&gt;如果这样还不够显然，我们可以换个角度：人群中一个人的检测结果为阳性，这个人是健康的概率却是 $P(H\vert +)=1-P(D\vert +)=0.835$。也就是说，一千个检测结果为阳性的人，其中大约 835 个人其实是健康的。&lt;/p&gt;

&lt;h2 id=&quot;箱子里的奖品&quot;&gt;箱子里的奖品&lt;/h2&gt;

&lt;p&gt;很多人都听过这样一个题目：&lt;sup id=&quot;fnref:3box&quot;&gt;&lt;a href=&quot;#fn:3box&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;有三个箱子，其中只有一个箱子里面有奖品。现在一个抽奖的人选择一个箱子打开。但是主持人并不是立刻打开箱子，而是打开了另外未被选择的两个里面其中没有奖品的一个箱子。
现在主持人问抽奖人：你要不要换一下你的选择？&lt;/p&gt;

&lt;p&gt;如果不使用 Bayes 理论，这个问题思考起来错综复杂，容易出错。下面的 Bayes 理论的计算却简单明了。&lt;/p&gt;

&lt;p&gt;使用的符号：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$P(Win\vert Switch,WrongBox)$：抽奖人第一次选择了没有奖品的箱子，并且选择切换自己的选择，然后获奖的概率。这个概率总是 1.&lt;/li&gt;
  &lt;li&gt;$P(Win\vert Switch,RightBox)$：抽奖人第一次选择了有奖品的箱子，并且选择切换自己的选择，然后获奖的概率。这个概率总是 0.&lt;/li&gt;
  &lt;li&gt;$P(Win\vert NoSwitch,RightBox)$：抽奖人第一次选择了正确的箱子，但是选择坚持第一次选择，然后获奖的概率。这个概率值为 1.&lt;/li&gt;
  &lt;li&gt;$P(WrongBox)$：在第一次选择中，抽奖人选择到错误的箱子的概率。这个概率是 2/3.&lt;/li&gt;
  &lt;li&gt;$P(RightBox)$：在第一次选择中，抽奖人选择到正确的箱子的概率。这个概率是 1/3.&lt;/li&gt;
  &lt;li&gt;$P(Win\vert Switch)$：抽奖人选择切换自己的选择并且赢得奖品的概率。这是需要计算的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;利用 Bayes 定理，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Win\vert Switch) = P(Win\vert Switch,WrongBox)P(WrongBox) + P(Win\vert Switch,RightBox)P(RightBox) = 2/3.&lt;/script&gt;

&lt;p&gt;简单的计算可以得到不改变主意并且获奖的概率，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Win\vert NoSwitch)= P(Win\vert NoSwitch,RightBox)P(RightBox) + P(Win\vert NoSwitch,WrongBox)P(WrongBox) = 1/3.&lt;/script&gt;

&lt;p&gt;所以显而易见了，当主持人问的时候，我们要选择切换。&lt;/p&gt;

&lt;h2 id=&quot;神经科学中的应用&quot;&gt;神经科学中的应用&lt;/h2&gt;

&lt;p&gt;在神经科学中，实验经常获得一些多次重复实验的放电记录。&lt;sup id=&quot;fnref:neuronspikes&quot;&gt;&lt;a href=&quot;#fn:neuronspikes&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;figcaption&gt;
神经科学中的一种 raster plot：横轴是时间，纵轴是第几次实验。例如纵轴是 1，我们按照横轴读过去，发现在某些时刻，这个神经元会放电，换到纵轴是 2，也就是第二次重复实验，发现跟第一次并不完全吻合。
&lt;/figcaption&gt;
  &lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/bayesian/raster_baselineandstim.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;由于这种内在的随机性，在理论上描述的时候，需要使用概率论。而我们又常常关心在给定刺激的时候放电的概率 $P(\mathrm{spike}\vert \mathrm{stimulus})$，或者在放电了存在刺激的概率 $P(\mathrm{stimulus}\vert \mathrm{spike})$。&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:set&quot;&gt;
      &lt;p&gt;比较严格的理解是把 A、B 理解成两组可能事件的集合，$P(A\cup B)$ 的意义就是发生事件即在 A 集合中，又在 B 集合中。这样对于理解 Venn diagram 有帮助。&amp;nbsp;&lt;a href=&quot;#fnref:set&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:condi2&quot;&gt;
      &lt;p&gt;如果我们这里面的 $B$ 事件其实是 $C\cap D$ 呢？那么我们有$P(A\vert (C\cap D)) P(C\cap D) = P(A\cap C \cap D).$&amp;nbsp;&lt;a href=&quot;#fnref:condi2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:raredisease&quot;&gt;
      &lt;p&gt;参考 Kevin Cahill 的 &lt;em&gt;Physical Mathematics&lt;/em&gt;，第 13 章.&amp;nbsp;&lt;a href=&quot;#fnref:raredisease&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3box&quot;&gt;
      &lt;p&gt;参考 Kevin Cahill 的 &lt;em&gt;Physical Mathematics&lt;/em&gt;，第 13 章.&amp;nbsp;&lt;a href=&quot;#fnref:3box&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:neuronspikes&quot;&gt;
      &lt;p&gt;https://praneethnamburi.wordpress.com/2015/02/05/simulating-neural-spike-trains/&amp;nbsp;&lt;a href=&quot;#fnref:neuronspikes&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/sciencecn/bayesian/&quot;&gt;贝叶斯统计、罕见疾病和抽奖问题&lt;/a&gt; was originally published by HLLM at &lt;a href=&quot;http://localhost:4000&quot;&gt;NEURONSTAR&lt;/a&gt; on May 10, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[回声定位]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/sciencecn/echolocation/" />
  <id>http://localhost:4000/sciencecn/echolocation</id>
  <updated>2015-03-22 00:00:00 +0000T00:00:00-00:00</updated>
  <published>2015-03-22T00:00:00+00:00</published>
  
  <author>
    <name>neuronstar</name>
    <uri>http://localhost:4000</uri>
    
  </author>
  <content type="html">
    &lt;h2 id=&quot;回声定位的原理&quot;&gt;回声定位的原理&lt;/h2&gt;

&lt;p&gt;回声定位是一种特殊的听觉形式，即动物发出超声波，超声波碰到障碍物后反弹，动物通过分析反弹的超声波的频率、波幅，来判断障碍物的距离、速度、类型、大小。这一技能被动物用于导航、捕食、探索环境。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/echo/echo1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;EL=SL+TS-2TL&lt;/script&gt;

&lt;p&gt;通常来说，人类能够听到的声音频率范围是 20Hz-20,000Hz，超过 20,000Hz 的声波即超声波，是人类听觉系统（耳蜗）不能感知的刺激。但是，很多动物能听到超声波。其中的典范就是齿鲸（包括喙鲸和海豚）和蝙蝠，它们能利用超声波进行定位，即回声定位。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/echo/echo2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/echo/echo3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;海豚与蝙蝠&quot;&gt;海豚与蝙蝠&lt;/h2&gt;

&lt;h3 id=&quot;海豚的发声器官和接受器官&quot;&gt;海豚的发声器官和接受器官&lt;/h3&gt;
&lt;p&gt;海豚通过挤压空气快速穿过鼻腔内的两对脂肪片而发出超声波（下图的绿色弧线），声音通过头部的脂肪传播。海豚通过下颌的脂肪接收返回的超声波（下图的红色弧线）。&lt;/p&gt;

&lt;figure&gt;
  &lt;figcaption&gt;
Image Credit: &lt;a href=&quot;http://commons.wikimedia.org/wiki/User:Malene&quot; target=&quot;_blank&quot;&gt;Malene Thyssen&lt;/a&gt;
&lt;/figcaption&gt;
  &lt;p&gt;&lt;img src=&quot;http://upload.wikimedia.org/wikipedia/commons/8/82/Delfinekko.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;蝙蝠的发声器官和接受器官&quot;&gt;蝙蝠的发声器官和接受器官&lt;/h3&gt;

&lt;p&gt;蝙蝠通过喉头、鼻腔发出超声波，也可以通过翅膀发出微弱的超声波。返回的超声波通过外耳廓进入蝙蝠的听觉系统。&lt;/p&gt;

&lt;h2 id=&quot;海豚和蝙蝠的捕食过程&quot;&gt;海豚和蝙蝠的捕食过程&lt;/h2&gt;

&lt;p&gt;海豚和蝙蝠的捕食过程可以分为三个阶段：搜索、趋近、捕食。在这三个阶段中，海豚和蝙蝠发出的声音频率不变，而信号间隔逐渐变短，是为了得到更精确的关于猎物的信息，最后阶段当他们非常接近猎物时，信号间隔降低为几毫秒，为“蜂鸣”信号（buzz）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/echo/echo5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;捕食者-猎物的互动和进化&quot;&gt;捕食者-猎物的互动和进化&lt;/h2&gt;

&lt;p&gt;在齿鲸中，有一对有趣的捕食者-猎物的例子，那就是虎鲸和海豚，他们都能够接受超声波信号，因此，以哺乳动物为食的虎鲸为了避免被海豚“听”到，在接近海豚时会关闭回声定位系统；而海豚在捕食鱼类时，为了避免被虎鲸“听”到而暴露自己，进化出了高于虎鲸听力上限的超声波范围。这就是 predator 和 prey 斗智斗勇的生存游戏。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/echo/echo6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;人类的回声定位&quot;&gt;人类的回声定位&lt;/h2&gt;

&lt;p&gt;有趣的是，最近的研究发现，回声定位不止存在于齿鲸和蝙蝠中，有的盲人也可以利用回声判断行进过程中的障碍物。&lt;/p&gt;

&lt;iframe width=&quot;960&quot; height=&quot;720&quot; src=&quot;https://www.youtube.com/embed/r9mvRRwu5Gw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;回声定位的物理&quot;&gt;回声定位的物理&lt;/h2&gt;

&lt;p&gt;海豚捕食小鱼的时候，需要定位精度&lt;strong&gt;至少&lt;/strong&gt;达到鱼的大小，这也就意味着海豚发出的声波的波长要小于小鱼的大小，例如 10 厘米。而蝙蝠需要捕食很小的昆虫，如果定位精度只有 10 厘米，显然是没法捕捉到昆虫的，因为它只能知道在昆虫在一个 10 厘米的范围内，却不知道昆虫具体在什么位置，因此它的超声波的波长&lt;strong&gt;至少&lt;/strong&gt;要达到小昆虫的大小，例如 1 厘米的飞蛾。&lt;/p&gt;

&lt;p&gt;比较巧合的是，海豚和蝙蝠的超声定位工作在相似的频率上。这背后的原因是声波波长和声速成正相关，即&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_s = \lambda f,&lt;/script&gt;

&lt;p&gt;其中 $v_s$ 是声速，$\lambda$ 是波长，$\lambda$ 是声波波长，$f$ 声波频率。&lt;/p&gt;

&lt;p&gt;由于海水中声速接近空气中声速的五倍，同样频率的声波，在海水中的波长大约是空气中波长的五倍。上面提到的海豚和蝙蝠超声定位虽然工作在相似的频率上，但是在空气中的蝙蝠可以探测到的猎物的尺度大约是海水中海豚探测精度的五倍。也就是说，虽然他们需要的精度不同，但是由于声速在两个环境中的差异，蝙蝠并不需要发出比海豚频率更高很多的声波。&lt;/p&gt;

&lt;figure&gt;
  &lt;figcaption&gt;
图中横轴是声波频率，左边的纵轴是探测猎物的大小。同样的频率下，蝙蝠可以探测更小。Madsen PT, Surlykke A. 2013 Vol. 28 no. 5, 276-283. &lt;a href=&quot;http://physiologyonline.physiology.org/content/28/5/276.long&quot; target=&quot;_blank&quot;&gt;Functional convergence in bat and toothed whale biosonars&lt;/a&gt;.
&lt;/figcaption&gt;
  &lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/echo/echo7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/sciencecn/echolocation/&quot;&gt;回声定位&lt;/a&gt; was originally published by HLLM at &lt;a href=&quot;http://localhost:4000&quot;&gt;NEURONSTAR&lt;/a&gt; on March 22, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[人工神经网络方法解方程]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/sciencecn/ann-ode/" />
  <id>http://localhost:4000/sciencecn/ann-ode</id>
  <updated>2015-08-31 00:00:00 +0000T00:00:00-00:00</updated>
  <published>2015-03-22T00:00:00+00:00</published>
  
  <author>
    <name>OctoMiao</name>
    <uri>http://localhost:4000</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;数值方法解方程大多要一个把方程或者函数参数化，例如改写成差分方程，变成步长等等的函数。例如我们要解这样一个方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dy(t)}{dt} = -y(t),&lt;/script&gt;

&lt;p&gt;其中初始条件 $y(0)=1$ 已知。&lt;/p&gt;

&lt;p&gt;一种普遍的手段是，我们可以把其中的待解函数参数化为 $y(t,\{n_i\})$，然后把方程重新写成&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dy(t, \{n_i\})}{dt}  + y(t,\{n_i\}) = 0,&lt;/script&gt;

&lt;p&gt;其中 $\{n_i\}$ 是用来参数化这个函数的参数。有了这个式子，我们会联想到最小二乘法。也就是说，如果我们的参数化完全正确的话，我们应该得到方程的左边的两项之和总是零，也就是 $\frac{dy(t, \{n_i\})}{dt}$ 和 $- y(t,\{n_i\}) $ 的没有偏离，即&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{waste} = \frac{dy(t, \{n_i\})}{dt}  + y(t,\{n_i\}) ,&lt;/script&gt;

&lt;p&gt;这个量总是零。然而数值上来讲，我们有一个更加简单的方法来让这个量在任意的 $t$ 总是零，方法就是借助最小二乘法的思路，利用一个凹函数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{cost} = \int dt ( \frac{dy(t, \{n_i\})}{dt}  + y(t,\{n_i\}) )^2.&lt;/script&gt;

&lt;p&gt;倘若 $\text{cost} = 0$，我们推断上面定义的 $\text{waste}$ 函数在任意的 $t$ 都为零。&lt;/p&gt;

&lt;h2 id=&quot;人工神经网络参数化&quot;&gt;人工神经网络参数化&lt;/h2&gt;

&lt;p&gt;下面我们引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。&lt;/p&gt;

&lt;p&gt;人的神经元有一个特点就是，存在激活和不激活两种状态，激活的话信号传递，不激活的话，信号被毙掉。数学上我们可以通过一个类似阶梯的函数来模拟这个行为，例如：&lt;/p&gt;

&lt;figure&gt;
  &lt;figcaption&gt;
阶梯函数图像，可以挑选出某些参数值。
&lt;/figcaption&gt;
  &lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/annode/Dirac_distribution_CDF.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;然而数值上来讲，这个函数有个非常大的缺点，就是导数不连续。尤其是对于我们想要解一个微分方程来说，用来参数化的函数里面包含导数不连续的部分，那是非常不方便的。&lt;/p&gt;

&lt;p&gt;所以我们会选取一个导数连续但是也具有类似行为的函数，比如一个 sigmoid 函数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{sigmoid}(t) = \frac{1}{1+e^{-t}}.&lt;/script&gt;

&lt;p&gt;这个函数的图像是&lt;/p&gt;

&lt;figure&gt;
  &lt;figcaption&gt;
sigmoid 函数，也具有挑选出某些参数值的功能。
&lt;/figcaption&gt;
  &lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/posts/annode/Logistic-curve.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;这个函数有个非常大的优势，就是如果我们把要求解的函数利用这个参数化，所有的导数全是可以解析求解的，对于数值计算是一个福音。&lt;/p&gt;

&lt;p&gt;我们可以模拟神经元可以被激活这种性质，来挑选出一些我们想要的参数，这个示例中用到的就是 sigmoid 函数。我们挑选一组参数 ${v_k}$, ${w_k}$ 和 ${u_k}$，对于一个函数 $y(t)$，我们可以用下面的方法来参数化，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(t)= y(0)+t \sum_k v_k f(t w_k+u_k).&lt;/script&gt;

&lt;p&gt;这样做的原因是在 $t=0$ 时，我们确保参数化的函数满足给定的初始条件（$t=0$ 时 $t \sum_k v_k f(t w_k+u_k)=0$）。而 $f(x)$ 使我们的启动函数，这里我们用 sigmoid 函数。也就是说，在某个时刻 $t_i$，当参量 $t w_k+u_k$ 比较大的时候，我们输入的 $t$ 才会起作用，否则我们的输入 $t$ 被压制。&lt;/p&gt;

&lt;p&gt;$w_k$ 是对启动函数的缩放，$u_k$ 是对启动函数的平移，而 $v_k$ 的作用是放大缩小函数值。可以设想，当我们的参数足够多的时候，我们可以用这个参数化来组成任何连续函数。&lt;/p&gt;

&lt;h2 id=&quot;cost&quot;&gt;cost&lt;/h2&gt;

&lt;p&gt;那么如何得到这些参数的值呢？我们一开始定义了这样一个量，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{cost} = \int dt ( \frac{dy(t, \{n_i\})}{dt}  + y(t,\{n_i\}) )^2.&lt;/script&gt;

&lt;p&gt;我们想要做的是得到什么的参数可以有 $\text{cost} = 0$ 的结果。&lt;/p&gt;

&lt;p&gt;然而我们不想做积分，所以自然的选择是离散化，选择一个序列 ${t_i}$，并且考虑&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{cost} = \sum_j \left(\frac{dy(t_j, \{n_i\})}{dt}  + y(t_j,\{n_i\}) \right)^2.&lt;/script&gt;

&lt;p&gt;当我们选到一组参数使得这个量为零的时候，我们的参数就是函数的正确的参数化（在当前的初始条件下的）。&lt;/p&gt;

&lt;p&gt;所以我们就把一个解方程问题转换成了一个最小化问题了，因为 $\text{cost}$ 的值越小，说明我们的参数就越接近满足&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dy(t, \{n_i\})}{dt}  + y(t,\{n_i\}) = 0,&lt;/script&gt;

&lt;p&gt;当然是在一个特定初始条件下，这里我们的初始条件是 $y(0)=1$。&lt;/p&gt;

&lt;p&gt;剩下的部分，就无需我多说了，只需要用任何你想要的方法，来把 $\text{cost}$ 最小化，得到的参数值们 $\{v_k\}$, $\{w_k\}$ 和 $\{u_k\}$ 带回到函数的参数化&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(t)= y(0)+t \sum_k v_k f(t w_k+u_k),&lt;/script&gt;

&lt;p&gt;这样我们就有了这个微分方程在当前初始条件下的解。&lt;/p&gt;

&lt;p&gt;上面我们离散化 cost 的过程中，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{cost} = \sum_j \frac{dy(t_j, \{n_i\})}{dt}  + y(t_j,\{n_i\}) )^2.&lt;/script&gt;

&lt;p&gt;里面的每一个时间点的输入都是一次对神经网络的训练，如同人的学习一样，经过多次训练，我们就可以掌握方程的这种行为。而所学习到的信息，存在了神经网络的参数中（也就是网络的结构中）。&lt;/p&gt;

&lt;h2 id=&quot;代码举例&quot;&gt;代码举例&lt;/h2&gt;

&lt;p&gt;以下是这个问题的 Python 代码，&lt;a href=&quot;http://nbviewer.ipython.org/github/NeuPhysics/sync-de-solver/blob/master/ipynb/neural-net.ipynb&quot;&gt;这里有一份带有全面说明的 IPython Notebook&lt;/a&gt;。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.optimize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.special&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expit&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Don't know why but np.asarray(v) doesn't work here.&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;fvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trigf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# This is a vector!!!&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fvec&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# For a given t, this calculates the value of y(t), given the parameters, v, w, u.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fvec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;fvec&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;trigf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#return 1/(1+np.exp(-x)) #&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;costTotal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;costt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;costt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;costt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;costt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;costTotalF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;costTotal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tlin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;initGuess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;costTotalF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initGuess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;SLSQP&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;为什么找这么多麻烦&quot;&gt;为什么找这么多麻烦&lt;/h2&gt;

&lt;p&gt;显然，上面的例子不仅可以用任何差分方法来解，而且有解析解。我们为什么要找这么多麻烦来做这样的参数化呢？&lt;/p&gt;

&lt;p&gt;或许我们要解决一个当前的状态依赖于全空间中的所有点的问题，这时候，我们就不好再用差分法了，因为要解出当前点我们需要知道所有的其他的点。上面这种一次性解出整个方程的方法就变得更加方便，因为我们解出方程并不是依赖于获得其他地方的信息，而是通过人工神经网络一次性猜到所有空间点的解。&lt;/p&gt;

&lt;p&gt;不过，本文开始讲的这种方法具有非常好的普适性。我们使用 ANN 是因为 Cybenko 在 1989 年证明了一个 sigmoid 可以作为很好的 universal approximator 来处理任意的 measurable functions。&lt;sup id=&quot;fnref:cybenko1989&quot;&gt;&lt;a href=&quot;#fn:cybenko1989&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;现在我们重新思考一下这个方法的普适性。倘若我们有个系统，可以用傅里叶展开，而且所幸我们只需要展开中的前 100 项就可以很好的来估算整个系统了。那么我们就把这个展开式写出来，然后取前 100 项，这样一样定义一个 cost，一样来找到使得 cost 最小化的参数，就可以求出傅里叶展开的系数们了。&lt;/p&gt;

&lt;p&gt;不过这个方法是不是足够有效，取决于我们所挑选的参数化形式。Kolmogorov 证明过如果选的足够好，我们可以用&lt;strong&gt;有限个&lt;/strong&gt;与 y 无关的函数来精确的还原 y。&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:cybenko1989&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://numsoltun.readthedocs.org/ann.html#universal-approximators&quot;&gt;Click here to find out the statement.&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:cybenko1989&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/sciencecn/ann-ode/&quot;&gt;人工神经网络方法解方程&lt;/a&gt; was originally published by HLLM at &lt;a href=&quot;http://localhost:4000&quot;&gt;NEURONSTAR&lt;/a&gt; on March 22, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Today I Learned]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/til/til/" />
  <id>http://localhost:4000/til/til</id>
  <updated>2015-03-21 00:00:00 +0000T00:00:00-00:00</updated>
  <published>2014-10-16T00:00:00+00:00</published>
  
  <author>
    <name>neuronstar</name>
    <uri>http://localhost:4000</uri>
    
  </author>
  <content type="html">
    

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/til/til/&quot;&gt;Today I Learned&lt;/a&gt; was originally published by HLLM at &lt;a href=&quot;http://localhost:4000&quot;&gt;NEURONSTAR&lt;/a&gt; on October 16, 2014.&lt;/p&gt;
  </content>
</entry>

</feed>
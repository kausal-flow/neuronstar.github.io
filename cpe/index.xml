<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Conditional Probability Estimation on NeuronStar</title><link>/cpe/</link><description>Recent content in Conditional Probability Estimation on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 21 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="/cpe/index.xml" rel="self" type="application/rss+xml"/><item><title>Conditional Probability and Bayes</title><link>/cpe/01.conditional-probability-and-bayes/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>/cpe/01.conditional-probability-and-bayes/</guid><description>The Bayesian view of probability is quite objective and also more general than the frequentist&amp;rsquo;s view. It doesn&amp;rsquo;t rely on repeatition of events.</description></item><item><title>Least Squares, Bootstrap, Maximum Likelihood, and Bayesian</title><link>/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</guid><description>Least squares, bootstrap, maximum likelihood, and maximum posterior leads to the same results in many cases.</description></item><item><title>EM Methods</title><link>/cpe/03.em-methods/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>/cpe/03.em-methods/</guid><description>Topics EM for Gaussian mixtures General EM algorithm Why does it work? Decomposition of log-likelihood into KL divergence and Relation between EM and Gibbs sampling</description></item><item><title>Variantional Inference Normalizing Flow</title><link>/cpe/04.variational-inference-normalizing-flow/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><guid>/cpe/04.variational-inference-normalizing-flow/</guid><description>Topics Variational Inference Normalizing Flow Variational Inference with Normalizing Flows</description></item><item><title>Review of Normalizing Flow</title><link>/cpe/05.normalizing-flow-review/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><guid>/cpe/05.normalizing-flow-review/</guid><description>Topics Normalizing flow Applications of normalizing flow Methods of normalizing flow Problems of normalizing flow</description></item><item><title>Deep AutoRegressive Networks</title><link>/cpe/06.deep-autoregressive-networks/</link><pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/06.deep-autoregressive-networks/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>MADE: Masked Autoencoder for Distribution Estimation</title><link>/cpe/07.made/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/07.made/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>MAF: how is MADE being used</title><link>/cpe/08.maf/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/08.maf/</guid><description>We discussed MAF (arXiv:1705.07057v4) last time: The paper did not explain how exactly is MADE being used to update the shift and logscale.
We will use the tensorflow implementation of MAF to probe the above question. Here is the link to the relevant documentation: https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow
Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>Summary of Generative Models</title><link>/cpe/09.summary-of-generative-models/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/09.summary-of-generative-models/</guid><description/></item><item><title>Energy-based Models</title><link>/cpe/10.energy-based-learning/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/10.energy-based-learning/</guid><description>We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>Energy-based Models 2</title><link>/cpe/11.energy-based-learning-2/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/11.energy-based-learning-2/</guid><description>We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>Energy-based Models 3</title><link>/cpe/12.energy-based-learning-3/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>/cpe/12.energy-based-learning-3/</guid><description>In the past two meetups, we have been discussing EBM from a computer scientist&amp;rsquo;s perspective.
In this discussion, we will discuss chapter XV of Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001</description></item><item><title>Energy-based Models 4</title><link>/cpe/13.energy-based-learning-4/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>/cpe/13.energy-based-learning-4/</guid><description>In this discussion, we will discuss the Pytorch Deep Learning Lectures by LeCun.</description></item><item><title>Energy-based Models 5</title><link>/cpe/14.energy-based-learning-5/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>/cpe/14.energy-based-learning-5/</guid><description>In this meetup, we will discuss Restricted Boltzmann Machine (RBM). We will cover the reason for introducing RBM and the training. At the end of the discussion, we will also cover some topics of Deep Boltzmann Machines.</description></item><item><title>LTD/LTP</title><link>/cpe/16.ltd-ltp/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>/cpe/16.ltd-ltp/</guid><description>In this meetup, we will discuss some key ideas related to biological neural network: LTP and LTD.</description></item><item><title>Predictive Coding Approximates Backprop along Arbitrary Computation Graphs</title><link>/cpe/15.predictive-coding/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>/cpe/15.predictive-coding/</guid><description>In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182</description></item><item><title>References for Probability Estimation Club</title><link>/cpe/00.references/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>/cpe/00.references/</guid><description>A list of references for our online discussions.</description></item></channel></rss>
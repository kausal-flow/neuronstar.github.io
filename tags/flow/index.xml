<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Flow on NeuronStar</title><link>/tags/flow/</link><description>Recent content in Flow on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 27 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/flow/index.xml" rel="self" type="application/rss+xml"/><item><title>MADE: Masked Autoencoder for Distribution Estimation</title><link>/cpe/07.made/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/07.made/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>MAF: how is MADE being used</title><link>/cpe/08.maf/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/08.maf/</guid><description>We discussed MAF (arXiv:1705.07057v4) last time: The paper did not explain how exactly is MADE being used to update the shift and logscale.
We will use the tensorflow implementation of MAF to probe the above question. Here is the link to the relevant documentation: https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow
Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>Summary of Generative Models</title><link>/cpe/09.summary-of-generative-models/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/09.summary-of-generative-models/</guid><description/></item></channel></rss>
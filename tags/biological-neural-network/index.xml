<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Biological Neural Network on NeuronStar</title><link>https://neuronstar.github.io/tags/biological-neural-network/</link><description>Recent content in Biological Neural Network on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 21 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://neuronstar.github.io/tags/biological-neural-network/index.xml" rel="self" type="application/rss+xml"/><item><title>Predictive Coding Approximates Backprop along Arbitrary Computation Graphs</title><link>https://neuronstar.github.io/cpe/15.predictive-coding/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/15.predictive-coding/</guid><description>In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182
Why? Feedforward-backprop usually has a loss function that involves all the parameters. Backprop means we need this huge global loss $\mathcal L({w_{ij}})$. However, it is hard to imaging such global loss calculations in our brain. One of the alternatives is predictive coding, which only utilizes local connection information.
In this paper (2006.04182), the author proves the equivalence of backprop and predictive coding on arbitary graph.</description></item><item><title>LTD/LTP</title><link>https://neuronstar.github.io/cpe/16.ltd-ltp/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/16.ltd-ltp/</guid><description>In this meetup, we will discuss some key ideas related to biological neural network: LTP and LTD.</description></item></channel></rss>
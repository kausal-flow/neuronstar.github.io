<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.68.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>人工神经网络方法解方程 | NeuronStar | NeuronStar | NeuronStar</title><meta name=author content="OctoMiao"><meta property="og:title" content="人工神经网络方法解方程"><meta property="og:description" content="引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。"><meta property="og:type" content="article"><meta property="og:url" content="/posts/articles/2015-03-22-ann-ode/"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-61051776-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=/posts/articles/2015-03-22-ann-ode/><link rel="shortcut icon" type=image/png href=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAABwlBMVEUAAAA/3MUK07UJ0rRk6NkC0rMAw5sAx6Ic1rs/3MYAlU0E0rQAhjMAyKIAiTkM07UJ07UAhC8m2L4o2L5O4Msn2L4AzKoAzq0128Mo2b8F0rQE0rMu2cAt2cAA0bI228MA0LAAy6kr2b8Q1LcF0rMH0rUAxp4t2b8c1ros2b8P1LcAzKsAzq4528Q02sIM07YAfyg93cUD0rMA0LEA0bFK4c0M1LcL07Uv2sEAzawY1rlJ4c0w2sEAzKkY1rqr+PQN07Yj170G0rQI07UF0bMt2cAF0rMC0bIL07UF0bMs2cAF0rMA0bIC0bIL07UF0rMA0bIA0bIA0bIO1LYr2b8N07Yp2L4F0rMF0rQJ07UA0bIC0bMA0bIH0rQK07UD0bMA0LAF0rMAyqYE0rMA0bIS1Lg228ID0bMM07YS1LcI07QL07UJ07UC0bMM07YG0rMS1Lc328IR1LcC0bIK07UM07Yo2L4P1LcL07UD0rMD0rMD0rMB0bIH0rQ23MMQ1LcD0rMN1LYB0LAJ0rQ33MMG0bMP1bgD0rMN07YH0rQH0rQ33MQF0rMD0rMN1LYo2L4F0rQG0rSA6tsA0bIB0bL////yxqscAAAAk3RSTlMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgnDXAUItvZKBAi6+vpMuf3++wEIAQcEBFn5AgR9UwMEoQLD/AEH4AIdAzha+UMEAQcc9AM4CAFSA9IC+4AEAc8yAwIEBAHOMQOBBLfLMAnFlAJNsa0LAAAAAWJLR0SVCGB6gwAAAAd0SU1FB+EBBBI4BduZ4woAAAF5SURBVDjLY2CAAUYmZgcWRydnVhc2dg4GTMAJlOdydZvs7sHtiU0FSJ7Hy3uyz2RfP24XNl4OLPJ8Xv5A+YBAsAo0M2DyQZMnY1fBL+AgCJXHrkIoWDgEJo9NhUioaBhCHqoiXExcAqZAMkIqckrU5MkoKqSjZeAmyMbIxSIZAFIRMCVOOl4epkAmQSERrwKOJMXkySl4rEhVSkufHDAZtyMzlDPdJweiyiN7UyRLIHsKQj5wMnowSIar5ExJQcjn5qnmqyFHlzqKL7EoQPMlphUaaL7EcGSBZiGKLzFUZGgVFSP5ElNFCVPpFORwhAeVto4uWEGwUJk3alSAVXhIR+tB7ShHTi5QkOIzpUKqEmKCPpODgReaipSUyVWl0tW6GEkWGhRBk2tq6wzrsSVqqPaGRqOmZmMTXWwqwNpbTFvbzMwtsGUMoPb2DqOmTksxK2xZLyhgck1XtzVIuw22zNszeXJvH0i7rR1G9gVlf/v+CRMnWaPYDgA74gwO0j2znAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAxNy0wMS0wNFQxODo1NjowNSswMTowME2yC4wAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMTctMDEtMDRUMTg6NTY6MDUrMDE6MDA877MwAAAAV3pUWHRSYXcgcHJvZmlsZSB0eXBlIGlwdGMAAHic4/IMCHFWKCjKT8vMSeVSAAMjCy5jCxMjE0uTFAMTIESANMNkAyOzVCDL2NTIxMzEHMQHy4BIoEouAOoXEXTyQjWVAAAAAElFTkSuQmCC><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/flatly/bulmaswatch.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous><link rel=stylesheet type=text/css href=https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4></script><link rel=stylesheet type=text/css href=https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:true,processEnvironments:true,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}});MathJax.Hub.Queue(function(){var all=MathJax.Hub.getAllJax(),i;for(i=0;i<all.length;i+=1){all[i].SourceElement().parentNode.className+=' has-jax';}});MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}});</script></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/>NeuronStar</a><div class="navbar-burger burger" data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Projects</a><div class=navbar-dropdown><a href=/categories/neuroscience/ class=navbar-item>Neuroscience</a>
<a href=/categories/sciencecn/ class=navbar-item>sciencecn</a>
<a href=/categories/statistical-learning/ class=navbar-item>Statistical Learning</a>
<a href=/categories/til/ class=navbar-item>til</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a itemprop=url class=navbar-item href=/esl/><span itemprop=name>ESL</span></a>
<a itemprop=url class=navbar-item href=/snm/><span itemprop=name>SNM</span></a>
<a itemprop=url class=navbar-item href=/search/><span itemprop=name></span></a><a itemprop=url class="navbar-item is-active" href=/posts/><span itemprop=name>Posts</span></a>
<a class=navbar-item target=blank href=https://github.com/neuronstar><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><main class=main--content aria-label=Content><div class="columns is-fullheight"><div class=column><section class="hero is-default is-bold"><div class=hero-body><div class="content container"><p>数值方法解方程大多要一个把方程或者函数参数化，例如改写成差分方程，变成步长等等的函数。例如我们要解这样一个方程：</p><p>$$\frac{dy(t)}{dt} = -y(t),$$</p><p>其中初始条件 $y(0)=1$ 已知。</p><p>一种普遍的手段是，我们可以把其中的待解函数参数化为 $y(t,\{n_i\})$，然后把方程重新写成</p><p>$$\frac{dy(t, {n_i})}{dt} + y(t,{n_i}) = 0,$$</p><p>其中 $\{n_i\}$ 是用来参数化这个函数的参数。有了这个式子，我们会联想到最小二乘法。也就是说，如果我们的参数化完全正确的话，我们应该得到方程的左边的两项之和总是零，也就是 $\frac{dy(t, \{n_i\})}{dt}$ 和 $- y(t,\{n_i\}) $ 的没有偏离，即</p><p>$$\text{waste} = \frac{dy(t, {n_i})}{dt} + y(t,{n_i}) ,$$</p><p>这个量总是零。然而数值上来讲，我们有一个更加简单的方法来让这个量在任意的 $t$ 总是零，方法就是借助最小二乘法的思路，利用一个凹函数，</p><p>$$\text{cost} = \int dt ( \frac{dy(t, {n_i})}{dt} + y(t,{n_i}) )^2.$$</p><p>倘若 $\text{cost} = 0$，我们推断上面定义的 $\text{waste}$ 函数在任意的 $t$ 都为零。</p><h2 id=人工神经网络参数化>人工神经网络参数化</h2><p>下面我们引入一种与人工神经网络相关的参数化方法，然后通过定义一个代价，最后最小化这个代价从而获得参数化中的各种参数，从而解出函数。</p><p>人的神经元有一个特点就是，存在激活和不激活两种状态，激活的话信号传递，不激活的话，信号被毙掉。数学上我们可以通过一个类似阶梯的函数来模拟这个行为，例如：</p><p>然而数值上来讲，这个函数有个非常大的缺点，就是导数不连续。尤其是对于我们想要解一个微分方程来说，用来参数化的函数里面包含导数不连续的部分，那是非常不方便的。</p><p>所以我们会选取一个导数连续但是也具有类似行为的函数，比如一个 sigmoid 函数，</p><p>$$\text{sigmoid}(t) = \frac{1}{1+e^{-t}}.$$</p><p>这个函数的图像是</p><p>这个函数有个非常大的优势，就是如果我们把要求解的函数利用这个参数化，所有的导数全是可以解析求解的，对于数值计算是一个福音。</p><p>我们可以模拟神经元可以被激活这种性质，来挑选出一些我们想要的参数，这个示例中用到的就是 sigmoid 函数。我们挑选一组参数 ${v_k}$, ${w_k}$ 和 ${u_k}$，对于一个函数 $y(t)$，我们可以用下面的方法来参数化，</p><p>$$y(t)= y(0)+t \sum_k v_k f(t w_k+u_k).$$</p><p>这样做的原因是在 $t=0$ 时，我们确保参数化的函数满足给定的初始条件（$t=0$ 时 $t \sum_k v_k f(t w_k+u_k)=0$）。而 $f(x)$ 使我们的启动函数，这里我们用 sigmoid 函数。也就是说，在某个时刻 $t_i$，当参量 $t w_k+u_k$ 比较大的时候，我们输入的 $t$ 才会起作用，否则我们的输入 $t$ 被压制。</p><p>$w_k$ 是对启动函数的缩放，$u_k$ 是对启动函数的平移，而 $v_k$ 的作用是放大缩小函数值。可以设想，当我们的参数足够多的时候，我们可以用这个参数化来组成任何连续函数。</p><h2 id=cost>cost</h2><p>那么如何得到这些参数的值呢？我们一开始定义了这样一个量，</p><p>$$\text{cost} = \int dt ( \frac{dy(t, {n_i})}{dt} + y(t,{n_i}) )^2.$$</p><p>我们想要做的是得到什么的参数可以有 $\text{cost} = 0$ 的结果。</p><p>然而我们不想做积分，所以自然的选择是离散化，选择一个序列 ${t_i}$，并且考虑</p><p>$$\text{cost} = \sum_j \left(\frac{dy(t_j, {n_i})}{dt} + y(t_j,{n_i}) \right)^2.$$</p><p>当我们选到一组参数使得这个量为零的时候，我们的参数就是函数的正确的参数化（在当前的初始条件下的）。</p><p>所以我们就把一个解方程问题转换成了一个最小化问题了，因为 $\text{cost}$ 的值越小，说明我们的参数就越接近满足</p><p>$$\frac{dy(t, {n_i})}{dt} + y(t,{n_i}) = 0,$$</p><p>当然是在一个特定初始条件下，这里我们的初始条件是 $y(0)=1$。</p><p>剩下的部分，就无需我多说了，只需要用任何你想要的方法，来把 $\text{cost}$ 最小化，得到的参数值们 $\{v_k\}$, $\{w_k\}$ 和 $\{u_k\}$ 带回到函数的参数化</p><p>$$y(t)= y(0)+t \sum_k v_k f(t w_k+u_k),$$</p><p>这样我们就有了这个微分方程在当前初始条件下的解。</p><p>上面我们离散化 cost 的过程中，</p><p>$$\text{cost} = \sum_j \frac{dy(t_j, {n_i})}{dt} + y(t_j,{n_i}) )^2.$$</p><p>里面的每一个时间点的输入都是一次对神经网络的训练，如同人的学习一样，经过多次训练，我们就可以掌握方程的这种行为。而所学习到的信息，存在了神经网络的参数中（也就是网络的结构中）。</p><h2 id=代码举例>代码举例</h2><p>以下是这个问题的 Python 代码，<a href=http://nbviewer.ipython.org/github/NeuPhysics/sync-de-solver/blob/master/ipynb/neural-net.ipynb>这里有一份带有全面说明的 IPython Notebook</a>。</p><p>{% highlight python %}
import numpy as np
from scipy.optimize import minimize
from scipy.special import expit
import matplotlib.pyplot as plt</p><p>def cost(v,w,u,t):
v = np.array(v) # Don&rsquo;t know why but np.asarray(v) doesn&rsquo;t work here.
w = np.array(w)
u = np.array(u)</p><pre><code>fvec = np.array(trigf(t*w + u) )  # This is a vector!!!
yt = 1 + np.sum ( t * v * fvec  )  # For a given t, this calculates the value of y(t), given the parameters, v, w, u.
return  ( np.sum (v*fvec + t * v* fvec * ( 1 -  fvec  ) * w ) + yt )   ** 2
</code></pre><p>def trigf(x):
#return 1/(1+np.exp(-x)) #
return expit(x)</p><p>def costTotal(v,w,u,t):
t = np.array(t)
costt = 0
for temp in t:
costt = costt + cost(v,w,u,temp)
return costt</p><p>costTotalF = lambda x: costTotal(np.split(x,3)[0],np.split(x,3)[1],np.split(x,3)[2],tlin)</p><p>initGuess = np.zeros(30)
result = minimize(costTotalF,initGuess,method="SLSQP&rdquo;)
{% endhighlight %}</p><h2 id=为什么找这么多麻烦>为什么找这么多麻烦</h2><p>显然，上面的例子不仅可以用任何差分方法来解，而且有解析解。我们为什么要找这么多麻烦来做这样的参数化呢？</p><p>或许我们要解决一个当前的状态依赖于全空间中的所有点的问题，这时候，我们就不好再用差分法了，因为要解出当前点我们需要知道所有的其他的点。上面这种一次性解出整个方程的方法就变得更加方便，因为我们解出方程并不是依赖于获得其他地方的信息，而是通过人工神经网络一次性猜到所有空间点的解。</p><p>不过，本文开始讲的这种方法具有非常好的普适性。我们使用 ANN 是因为 Cybenko 在 1989 年证明了一个 sigmoid 可以作为很好的 universal approximator 来处理任意的 measurable functions。<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>现在我们重新思考一下这个方法的普适性。倘若我们有个系统，可以用傅里叶展开，而且所幸我们只需要展开中的前 100 项就可以很好的来估算整个系统了。那么我们就把这个展开式写出来，然后取前 100 项，这样一样定义一个 cost，一样来找到使得 cost 最小化的参数，就可以求出傅里叶展开的系数们了。</p><p>不过这个方法是不是足够有效，取决于我们所挑选的参数化形式。Kolmogorov 证明过如果选的足够好，我们可以用<strong>有限个</strong>与 y 无关的函数来精确的还原 y。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=http://numsoltun.readthedocs.org/ann.html#universal-approximators>Click here to find out the statement.</a> <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><br></div></section></div></div></main></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://neuronstar.cc>NeuronStar</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>.
<strong>love</strong>.<br><a class=tag href=/index.xml>Feed</a>
<a class=tag href=/algolia.json>JSON Data</a>
<a class=tag href=/search>Search</a></p></div></div></footer></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>